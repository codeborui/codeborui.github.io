---
layout: post
title:  "Designing Data-Intensive Applications读书笔记(8)"
categories: DDIA
tags:  读书笔记 分布式
author: Borui
---

* content
{:toc}

# The Trouble with Distributed Systems
分布式系统和单机系统有很大的差异,分布式系统会面临很多新的故障问题.

## 故障与部分失败
单机的程序,如果硬件没有故障,那么相同操作下产生相同的结果.如果硬件出现了故障,那么程序将无法运作.因此对于运行编写良好程序的单机应用,要么完好的工作,要么彻底无法提供服务,不会存在中间的状态.在计算机的设计中有一个深思熟虑的设计:如果内部错误发生了,我们宁愿死机也不能返回错误的结果,因为处理起来,错误的结果很难处理也很困扰人.计算机将底层不确定都屏蔽掉,始终对应用层提供确定性.

在分布式系统里,消息在网络中传播所花费的时间是不确定的,而多个节点故障发生也是不确定的,因此这些不确定性加上部分失效导致分布式系统很难去处理这些问题.

### 云计算与超级计算
有两种理念来建立大规模的计算系统.
1. 一种理念是高性能计算(high-performance computing (HPC)).超级计算机拥有成千上万个cpu被用于计算敏感的科学任务,例如天气预报或者分子动态.
2. 一种是云计算,涉及到多用户的数据中心,通过以太网或者ip连接的商品电脑,弹性的资源分配以及计量计费.
3. 传统企业的数据中心,介于这两种之间.

在超级计算机上,一个作业不时的将当前的计算状态持久化到存储系统上,如果一个节点失败了,通常会将整个集群停下,在故障节点恢复后,计算工作通过最近的checkpoint继续进行.因此超级计算机更像是一个单机计算机,将部分失效演变成完全失效.但是我们更关注互联网服务,这跟超级计算机区别很大:
1. 许多互联网相关的应用是在线的,这意味着他们需要时刻和用户做低延迟的交互.将整个集群停下来不对外提供服务这是不可接受的,但是离线任务则完全没有问题.
2. 超级计算机通常运行在特殊的硬件上,通常每个节点相当可靠,节点间通过共享内存和远程直接内存访问设备(RDMA)来通信.而云服务里的节点都是商品机器,通过规模经济来使用较少的花销获取同等的性能,但是错误率很高.
3. 大型数据中心的网络通常预计于ip或者以太网,通过Clos拓扑结构提供高速双工带宽.超级计算机通常使用特殊的网络拓扑,例如多维网格或者环状,在已知的通信模式下为HPC提供更好的性能.
4. 系统越大,某个组件就更有可能产生故障,随着时间流逝,故障被修复新故障产生.但是在一个上千节点的系统里,假定故障随时发生是合理的.如果错误处理策略都只是简单的放弃处理,那么大型系统将花费大量的时间来故障恢复而不是做有用的事情.
5. 如果一个系统能够容错失效节点并且作为一个整体对外坚持提供服务,对于运维来说这是很有用的特征.
6. 物理分布式部署环境下(将数据放置在离用户更近的地方来降低访问延迟),通信通常都是经过因特网环境,对比局域网会更慢更不可靠.而超级计算机所有节点都被放置在一起.

如果我们想使得我们的分布式系统工作,我们必须接受**部分失效**和**建立容错机制**.换句话说,我们需要在不可靠的组件上建立可靠的系统.(这个可靠是有限度的,不可能十分完美的可靠).直观上来说,一个系统的可靠程度,取决于系统中最不可靠的组件,事实并非如此:
1. 纠错码允许数字信号通过一个偶尔会导致部分位错误的通信信道来传输准确的信息.例如无线网络中存在无线电干扰.
2. IP协议是不可靠的,会丢包延迟重复和重排序.但是tcp协议在ip基础上提供了更可靠的传输层.它能保证丢包被重发,重复消息被消除,接受到的消息顺序和发送一直.

尽管系统可以比它所依赖的组件更可靠,但是可靠性是有限度的.纠错码只能纠正少量的单字节错误,TCP也没有办法解决延迟问题.这些系统并不完美,但是通过屏蔽底层的一些错误,遗留的错误很容易被定位和解决.

## 不可靠网络
我们重点是关注无共享分布式系统,系统内的机器通过网络互联,网络也是系统间通信的唯一手段,每个机器拥有自己的内存和硬盘,但是机器之间无法相互访问内存和硬盘,只能通过网络访问彼此的服务接口.无共享不是构建系统的唯一方式,但这已经成为构建互联网服务的主导方式:
1. 因为不需要特殊的硬件,因此相对便宜,能够利用商业级别的云计算服务.
2. 通过多个地理隔离的分布式数据中心进行冗余,能获取高性能.

互联网和大部分数据中心的内部网络(通常是以太网),都是异步包网络(asynchronous packet networks).这种网络环境下,网络不提供保证数据包什么时间到达目的地,数据包是否能到达,如果你发送了请求然后等待响应,可能会有很多情况发生:
1. 你的请求丢失了(由于忘记插网线了)
2. 你的请求正在排队,然后晚些时候会被发送(获取是因为网络负载比较重)
3. 远程节点失效了
4. 远程节点临时停止响应了(由于长时间的GC),但后续会恢复工作
5. 远程节点响应了, 但是响应结果丢失在网络里了(或许由于交换机配置错误)
6. 远程节点响应了,但是响应结果被延迟了,晚些会发送(由于负载较高导致)

发送方甚至不清楚消息到底发送出去没.通常通过**超时**机制来解决.但实际上即使是超时了,你也不清楚到底发生了什么.

### 实际中的网络问题
网络故障时刻存在,即使是在公司维护的数据中心里,人为配置错误是主要的诱因.而共有网络里,可能交换机软件升级导致了数据延迟,鲨鱼咬断了海底光缆,甚至出现接收不到数据,但是能发送数据,因为网络链路不保证双工运行.

**网络分区**:当网络的一部分和另一部分无法连接,这被称为网络分区或者网路断裂.我们统称为网络故障.

处理网络故障并不意味着要容忍故障:如果你的网络相当可靠,那么处理方式可以是简单的将错误信息反馈给用户.但是你需要了解系统面临网络故障时的反应,以及需要确保系统能够从网络故障里恢复.

### 检测故障
需要系统需要自动的检测故障节点.
1. 负载均衡器需要停止将请求发送到僵死的节点.
2. 单主复制的分布式系统里,如果主节点失败了,其中一个从节点需要成为新的主节点.

然而网络使得检测故障节点很困难,在一些特定的场景下,可能会得到一些直白的反馈来告诉你节点是否还在工作:
1. 如果你能连接节点所在的机器,但是目标端口没有在监听(这意味着进程挂掉了),那么操作系统能够通过回复RST或者FIN包来关闭或者复用TCP连接.但是如果处理流程中进程崩溃了,你并不知道到底处理了多少数据.
2. 如果节点进程崩溃了(被管理员杀死进程了),但是节点所在的操作系统还在运行,然后通过脚本通知其他节点消息,使得其他节点不必等待超时.HBase就是这么处理的.
3. 如果你有权限连接到数据中心交换机的管理界面,就可以在硬件层面获取链接错误信息.但是如果你通过互联网连接,或者是共享的数据中心,没有访问权限,或者由于网络问题你无法访问管理界面,这就不可行了.
4. 如果一个路由器确定你想访问的ip地址是不可达的,会返回给你一个ICMP的目标不可达的包反馈.但是路由器也没有魔法能力来进行故障检测,它和其他网络组件服从一样的限制.

快速获悉远程节点宕机是很有用的,但你不能指望它.通常情况下,当发生故障时,你会什么响应也没有.你可以重试若干次等到超时,然后猜测远程节点已经宕机了.

### 超时和无限时延
如果超时是我们唯一检测故障的手段,那么超时时间应该设置多久呢?很不幸没有简单的答案.
1. 设置过长,节点被认为已经挂掉前需要一直等待,用户要么等要么看到错误信息.
2. 设置过短,检测速度很快,但是有很大概率会错误的认为节点已经宕机,也许只是偶尔那个时刻机器变慢了.

过早的声明一个节点失效了是有问题的,这会造成同样的操作被执行两次.而且当节点被声明失效了,那么其他节点将会接替该节点的工作,这会给其他节点增加负担.如果当前系统正在经历高负荷,而过早地声明一个失效了会加重这种境地.通常情况下节点都只是变慢了而没有完全是失效,这样一来会造成级联故障,所有节点都发觉其他节点因为变慢而声明他们都宕机了,于是整个系统都不工作了.

当网络延迟时间是有保证的时候,我们可以设定一个合理的超时时间.然而目前的异步网络遵循的是尽快的发包,但不保证发送时间的上限.
#### 网络拥塞和排队
1. 如果一些不同的节点同时往一个目标节点发包,网络交换机必须对他们进行排队,然后一个一个把它们推送到目标节点.当网络链路繁忙时,包必须等待,我们称之为网络拥塞.如果排队的包太多了,后来的包就会被丢弃需要被重发,尽管此时网络其实是好的.
2. 当包到达了目标机器,但是当前所有的cpu都在忙碌,那么到达的请求就需要在操作系统里排队,等待应用程序来处理.这取决于机器的负载程度,等多久是没有限制的.
3. 在虚拟化环境里,一个运行的os通常会暂停几十毫秒来等待其他虚拟机运行在cpu上.在这个阶段里,os没有办法消费任何网络请求,到达的数据统统都被虚拟机排队,这进一步增加了延迟.
4. TCP执行**流量控制**,也叫作**拥塞避免或者背压**.为了避免加重网络链路或者目标节点的负载,会限制发送节点的速度.这意味着发送时也需要额外的排队.

TCP会在一定超时范围内,将尚未确认的包认为已经丢失,超时时间通过之前观察到的往返之间来估算.丢失的包被自动重新发送,尽管应用层看不到丢包和重发,但是能看到的直观的结果就是时延.

当系统接近最大容量时,排队时延会非常大.拥有大量备用容量的系统可以很快的消耗队列,然而在高度被利用的系统中,长队列很快就会排起来.

在公有云和多租户机房里,资源是被许多消费者共享的.批量任务例如MapReduce很容易就把网络链路吃满.由于无法控制共享资源中其他用户的资源使用,如果你周围的节点使用了很多资源,那么网络延迟时间更加波动.

**在这种环境下,只能摸索着选择超时时间.通过较长时间内,跨多台机器之间来观察网络往返的时间分布,最终来决定延迟的可预期变化.然后再考虑应用程序的特征,在故障检测延迟和过早超时风险之间进行权衡.更好的方法就是不去配置固定超时,而是根据响应时间动态的调整超时.TCP就是这么做的.**

####TCP和UDP
一些对延迟敏感的应用会采用UDP,而不是TCP.这其实是在可靠性和延迟波动间做出的权衡.因为udp不会进行流量控制也不会重发丢失的包,因此避免了一部分网络延迟.udp适用于丢包延迟数据无所谓的场景,例如ip电话.

### 同步vs异步网络
如果分布式系统是部署在**最大时延固定**并且**不丢包**的网络环境下,问题将简单的多.那么为什么我们不部署在这种硬件环境呢?这里需要对比数据中心的网络和传统固定线路的电话网络.

当我们通过电话网络打电话时,首先需要建立一个**电路**:一个固定的,保证足够带宽,贯穿两端的线路,这个电路直到通话结束才关闭.这种网络是同步的,传输中的数据不需要排队,带宽固定因此时延固定,我们称之为**有界时延**.而传统tcp不同,一个电路具有固定带宽则独立使用,而tcp则是尽可能的使用网络带宽,当进行传输大小变化的数据块时,会尽量在最短的时间发送出去,并且当tcp连接空闲时是不占用带宽的.以太网和IP协议都是包交换协议,并没有电路的概念.

那么为什么会这么选择呢,这是为了**突发流量**做出的优化,因为打电话期间每秒的流量是固定的,而对于互联网来说,时刻发送的数据都是不固定的,因此需要尽可能的发送出去.如果我们通过电路传输数据,就需要提前申请带宽:如果申请小了,那么传送的速度太慢了,导致网络能力被浪费.如果申请的多了,电路建立不起来(因为没有足够的带宽支持).因此采用电路传送突发的流量,会导致网络能力没有被充分利用从而速度很慢.像TCP会动态的调整传送速度来利用网络的能力.

也有混合两种网络的网络,叫做ATM.无线带宽技术有些类似:它实现了链路层上端到端的流量控制,从而减少了网络排队的需求,但是仍然会经历链路拥塞的延迟.利用Qos(quality of service,包的优先级和调度)和接纳控制(admission control, 限速的发送方),能够在包网络中模拟电路交换,从而提供有界的延迟.但是在多租户数据中心和公有云,或者互联网通信场景下,这种服务质量还没有启用.

#### 延迟和资源利用
更一般的讲,我们可以吧变动的延迟看作是动态资源分区的结果.电路交换是一种**静态分配**方式,就算只有一通电话在拨打,其他的带宽也没有被利用.包交换网络是一种**动态分配**方式,数据被尽可能快的发送出去,这种方式有排队的缺点,但是能够最大化利用带宽.

在cpu利用上有相同的境遇.如果在多线程中动态的共享cpu核,那么线程之间需要互相等待和排队,因此挂起时间无法预测.但是硬件的利用率要比你给线程分配固定数量的cpu周期要好.更好的硬件利用率是采用虚拟机的动机.

在静态资源分配下,延迟是有保证的.但是也降低了利用率,换言之就是代价太大.而动态资源分配下,资源利用率提高了,但是延迟也变得不可控.

索引,变化的延迟不是自然产生的,而是代价/收益权衡的结果.