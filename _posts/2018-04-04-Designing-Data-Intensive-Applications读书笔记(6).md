---
layout: post
title:  "Designing Data-Intensive Applications读书笔记(6)"
categories: DDIA
tags:  读书笔记 分布式
author: Borui
---

* content
{:toc}

# Partitioning
当数据集非常非常大,并且我们需要非常高的吞吐量,我们就需要对数据集进行分片.每份数据都唯一的属于某个数据分区,每个数据分区都是整个大数据集的一小部分.

对数据集进行分区主要是为了可扩展性.不同的分区可以放置在无状态集群的不同节点上,于是一个大的数据集就被放置到许多不同的磁盘上,查询操作被分摊到不同的cpu上.

## Partitioning and Replication
分区往往和副本复制在一起工作,最终结果就是不同分区的不同副本分布在不同的节点上.这意味着,一条记录属于一个指定的分区,并且通过存储在不同的节点上来进行容错.

而一个节点上同样存储有不同的分区数据.如果是一个单主架构,那么分区+复制架构如图:
![Figure 6-1. Combining replication and partitioning: each node acts as leader for some partitions and follower for other partitions.](https://raw.githubusercontent.com/codeborui/codeborui.github.io/master/img/5.jpg)
每个分区的leader被分到一个节点上,它的follower被分配到其他节点上,因此每个节点可能是某个分区的leader,也可能是某个分区的follower.

## Partitioning of Key-Value Data
分区的目标是想将数据和查询尽可能的均衡的分布在节点上.这样可以得到线性的性能提升.但是如果分配不均匀,例如部分分区分配的数据量过多,则为产生**倾斜**,极端情况数据都分派到一台节点上,拥有不成比例的高负载的分区称为**热点**.最简单的方式,就是将节点随机分配,但是这种方法有个巨大的缺点,就是当你想要读取特定数据时,你找不到他在哪个分区上.

### Partitioning by Key Range
一种分区方式是,将连续范围的key分配到一个分区里.key的分布不一定是均匀的,因为数据并不一定均匀分布.为了使得数据最终能够均匀分区,因此分区的边界需要根据数据来定.分区的边界可以通过管理员来手工指定,也可以自动来选择.在每个分区里,我们会保持key的有序性,因此我们可以很方便的进行范围查询操作,也可以把kye当做组合索引来查询相关数据(例如key的值是通过年-月-日-时-分-秒这种形式).

不过这种方式的劣势也很明显,就是某种特定的查询模式将导致热点的出现,例如我们的key是时间戳形式,那么我们将频繁写入和查询最近时间戳所在的分区上,而其他分区则都是空闲状态.为了避免这种情况,你需要小心设计key值,key的开始元素可以根据实际情况选定特定分类,这样写入和查询就可以均匀分不到不同的分区上,只是获取某些数据时,需要合并各个分区查询的结果.

### Partitioning by Hash of Key
由于存在数据倾斜和热点问题,很多数据库采用hash函数来决定数据的分区.一个好的hash函数能使得倾斜的数据得到均匀的分布.由于只是为了分区,因此key值不需要被强加密,因此md5或者FowlerNoll–Vo函数都可以被采用.许多语言内置了简单hash函数,但不见得适用,因为同一个key值在不同的线程里可能都会有不同的hash值.当选定一个hash函数后,就可以给不同的分区指定不同的hash范围了.分区的边界可以是均匀间隔的,也可以是伪随机的(一致性hash).

一致性hash通过随机选择分区边界来避免中心控制和分布式一致性问题.事实上这种方式效果并不好,很多数据库的文档还称一致性hash其实是不准确的.

不过,通过这种方式,我们没有办法做key的范围查询了,并且分区内的key也不是有序的了.想做范围查询,就需要查询各个分区然后组合结果返回.

Cassandra提供了一种组合两种分区策略的方式,叫做复合主键.复合主键包含若干数据列,只用key的第一部分来进行hash分区,其他的列则用来排序.这样一来虽然无法通过第一列来进行范围查询,但是如果通过为第一列指定特定值,那么通过其他列就可以实现范围查询.

### 缓解热点
在极端情况下，所有的读写查询都针对一个key到时候，仍然会有热点产生。例如在社交网络里的大v，发表一篇文章，将会得到上百万的读操作。

对于存储系统来说这很难自动去解决数据倾斜问题，因此需要应用层去做。例如在热点key的首尾加上随机数使其被hash到不同的分区里去。但是这会带来一些问题。
1. 读取操作需要从各个分区读取，然后再汇总，增加了工作量。
2. 应用层需要跟踪热点key，从而针对必要的key值进行这种操作。

## Partitioning and Secondary Indexes
我们的数据系统并不总是简单的kv系统，只需要通过主key就可以获取所有的信息。我们还需要一些辅助索引来帮助查询。对于关系型数据库和文档型数据库来说，辅助索引很重要。但是对于某些kv系统（例如hbase）由于这会增加系统复杂度而选择抛弃辅助索引。辅助索引有两种实现方式，一种是基于文档的，一种是基于单词的。

### Partitioning Secondary Indexes by Document
基于文档的辅助索引，本质上就是每个分区都是相互独立的，只维护自己分区的辅助索引，因此这种方式又被称为**本地索引（local index）**。但是这种索引方式写入时只需要每个分区维护索引，读的时候就需要从多个分区获取数据并进行合并，这种操作被称为**scatter/gather**， 这使得读取操作很复杂。尽管可以并行从多个分区读取，但仍然存在尾部延迟放大问题。![Figure 6-4. Partitioning secondary indexes by document.](https://raw.githubusercontent.com/codeborui/codeborui.github.io/master/img/6.jpg)

### Partitioning Secondary Indexes by Term
和本地索引不同，我们可以建造一个**全局索引（global index）**，但是我们不能把全局索引保存在一个节点上，这样就会产生性能瓶颈从而违背了分区的初衷。因此全局索引也需要被分区，只不过分区方式和主key不一样。
![Figure 6-5. Partitioning secondary indexes by term.](https://raw.githubusercontent.com/codeborui/codeborui.github.io/master/img/7.jpg)

全局索引分区方式可以类似主key的分区方式，可以基于key的范围也可以基于hash。基于范围可以方便根据辅助索引做范围查询。基于hash则可以得到更好的负载分布。全局索引的好处在于读取操作只需要读取索引指定的分区，劣势在于写入操作会变得复杂，写入一行记录或一篇文章需要更改多个索引分区。

理想情况下，索引应该和数据的更新保持同步。但是对于全局索引这会涉及到多个分区的分布式事务，并不是所有的数据系统都支持。因此实际操作中，辅助索引的更新通常都是异步的。

## Rebalancing Partitions
从集群中的一个节点迁移数据到另一个节点的操作被称为再调整。最小的要求是：
1. 重整后，各个节点上的数据仍然是均匀分布的。
2. 当调整期间，系统必须能够继续接受读写请求。
3. 除了必要的数据外没有额外的数据需要迁移，从而保证调整尽可能的快并减少I/O操作。

### 再调整的策略
之前我们采用hash的时候，也是为hash值指定边界而不是直接对hash进行取模。因为当分区增加或者减少时，取模的结果将和调整前相差很多，从而导致大量的数据迁移，成本太高。

### 固定分区数量（hash）
我们可以指定超过节点数的分区个数，然后若干个分区指派到一个节点。当增加一个节点的时候，可以将其他的节点上的若干分区指派到新的节点上，从而达到负载均衡。当减少节点的时候，理论如此。
![Figure 6-6. Adding a new node to a database cluster with multiple partitions per node.](https://raw.githubusercontent.com/codeborui/codeborui.github.io/master/img/8.jpg)
这个过程中，只有部分分区的数据需要被迁移，分区的数量没有更改，key到分区的映射也没有更改，更改的只是分区到节点的分配。由于迁移的数据量较多并不是瞬时就能迁移完成的，因此在迁移过程中，分区旧的映射节点仍旧能够接受读写请求。这里可以理解，没有迁移完成的key仍旧由原有节点提供服务，迁移完成的节点由新的节点提供服务，而迁移中的节点可以告知客户端正在迁移。可以通过redis的cluster集群实例来理解：[Redis Cluster 分区实现原理](http://blog.jobbole.com/103258/)

这个方法还有一个优势，就是可以根据不同的硬件能力，给不同的节点分配不同数量的分区。实际中很多系统都是采用固定数量的分区之后就不在更改了，理论上可以分裂或者合并分区，但是固定的分区数量实现上更简单。最好一开始就选定可能的最大值，这需要你预判数据的增长，当然分区也有管理成本因此不适宜设置成太大的值。

当数据的规模非常大且持续增长的时候，分区的数量很难固定下来，因为每个分区包含固定比例的总数据量，随着数据量的增加，每个分区包含的数据量也将增加。如果分区数据量过大，那么重新分配和节点恢复成本就会很大。但是如果每个分区数据集过小，分区数量就会增加，这会增加管理的成本。因此想让数据分区大小合适是很困难的。

### 动态分区（range）
对于使用key的范围来分区，使用固定的分区数量就不太合适了。因为key的范围一旦预估错误，就会导致有的分区包含全部数据同时其他分区时空的，而重新调整分区配置成本太高。因此对于使用key的范围来分区的数据库，例如Hbase会动态的创建分区。当一个分区的大小增长到预先设置的大小时，就会进行分裂成两个分区，各包含一半数据。如果某个分区大量数据被删或者数据量少于阈值，则会合并分区成一个分区。这有点儿类似B数的操作。

**在单机上，有hash，lsm-trees和b-tree索引，在分布式环境则主要是hash和key-range（类似b-tree）**

每个分区归属于一个节点，每个节点上存在若干分区。当一个分区过大时会被分裂，分裂后的一个分区会被迁移到其他节点来平衡节点的负载。动态分区的好处就是可以根据数据量来制定分区的数量。不过对于一个空的数据集这会导致只有一个分区，为了解决这个问题需要进行配置来初始化分区，这被称为**pre-splitting**，这个操作需要对数据可能的分区结构有一个准确的预判。当然动态的分区不仅适用于根据key范围来分区，也适用于hash分区的数据集。

### 正比节点大小
动态分区下，分区的数量和数据量大小成正比。分裂和合并操作的存在使得分区的大小保持在最大值和最小值之间。在固定分区下，分区的大小和数据集的大小成正比。无论哪种情况，分区数量都和节点数量没有直接关联。于是第三种选择，就是使得分区的数量正比于节点数量，也就是说每个节点有固定的分区数量。当节点的数量不变的话，随着数据量的增加每个分区大小也会随着增长。而当节点数量增加分区的大小就会变小。

当一个新节点加入集群，它随机选择固定数量的现存分区进行分裂，分裂后的分区一半保留在原节点，另一半由新的节点监管。随机的方法可能会导致分裂不均匀，但是当平均分配到较大数量的分区上时，新的节点会得到一个相对合理的负载。

随机选择分区的边界需要基于hash的分区方式。实际上这种方式最接近原始的一致性哈希的定义。

### 自动化or手工
重分配是自动化还是手工完成呢？这两者之间有一个梯度，完全自动化和完全手工化。实际上很多数据库建议自动化的分区分配，但是需要管理员在分区分配生效前提交分配操作。完全自动化当然是很方便，但是结果是不可预期的，重新分配本身就是成本很高的操作，因为需要重新路由请求和迁移大量数据在节点之间，如果操作不当就会造成严重的后果。尤其是配合故障检测，会导致错误产生。因此最好配合人工去操作。

## Request Routing
由于数据分区了，因此客户端想要获取某个分区时怎么知道数据在哪个分区呢？这被称为服务发现，不单单是在数据库系统中。
1. 运行客户端连接任一一个节点（通过轮询来负载均衡）。如果该节点保留有该数据，则直接处理请求。否则将请求转发到合适的节点，然后接收到响应再把响应返回给客户端。
2. 所有的客户端请求先发送到路由层，路由层来决定哪个节点应该处理请求然后将请求直接发送到节点。路由层并不处理任何请求，只是用来做分区发现负载均衡的。
3. 需要客户端来知道分区的分配，从而直接将请求发送到合适的节点。
![Figure 6-7. Three different ways of routing a request to the right node.](https://raw.githubusercontent.com/codeborui/codeborui.github.io/master/img/9.jpg)

所有这些方法有一个问题，就是做路由选择的组件如何知道分区到节点的变化呢？有一些协议可以在分布式系统得到一致性但是它们很难实现。

许多分布式数据库依赖于一个独立的协调服务，例如zookeeper来保持对集群元数据的记录。zk上保持有分区到节点的映射，其他的路由选择组件通过订阅zk来获取这些信息，当有分区关系更改时，zk可以通知路由选择组件。
![Figure 6-8. Using ZooKeeper to keep track of assignment of partitions to nodes.](https://raw.githubusercontent.com/codeborui/codeborui.github.io/master/img/10.jpg)

部分数据库使用**gossip protocol**。请求被发送到任意一个节点上，然后节点负责将请求转发到合适的包含请求分区的节点上。这种方法增加了数据节点实现的复杂度，但避免了外部依赖协调服务。

对于路由层和发送到任意节点这两种方法，无论哪种都需要客户端找到连接的ip地址。由于地址的变更不像分区到节点的变化那么快，因此可以通过dns来加速查询。

### 并行查询
之前文章都只是提到简单的读写单个key，实际上**大规模并行处理（massively parallel processing——MPP）**的关系型数据库在查询操作上更加复杂。

## 总结
本章主要描述了各种分区的方法，当存储数据量较大并且单节点处理数据不再适用的时候，就需要进行分区。分区的目标在于将数据和查询负载分布到多台机器上，避免热点（高负载节点）的存在。这需要选择合适的分区模式以及重调整分区。

有两种主要的分区方法：
1. 基于key范围分区。这里key值是排序的，一个分区包含从小到大的全部key值。排序使得客户端做范围查询成为可能，但是也容易造成热点。这种方法下，分区重分配通常采用将范围分裂成两个子范围的方式。
2. 基于hash分区。将每个key应用hash函数，然后每个分区包含一定hash范围。这种方式破坏了分区内部的有序性，使得范围查询不是很高效，但是使得分区分布更加平均。当时使用这种方式分区时，重新调整通常是采用固定的分区数量，然后将若干分区指派到每个节点上，然后调整时需要将整个分区内容进行迁移。当然也可以采用动态分区方式。

除此之外，还可以采用混合方法，例如使用组合key。使用key的一个部分进行hash分区，其他部分用于排序。

除了针对主key分区，还可以针对辅助索引进行分区。主要有两种方法：
1. 文档分区索引，又叫本地索引。索引和分区数据存在一起，因此写时只需要修改一个分区内容，但是读取时则需要scatter/gather全部分区。
2. 基于词汇索引，又叫全局索引。索引值包含所有分区的主键。当写入文档时，需要修改多个分区上的索引记录，但读取时只需要读取一个分区上的索引记录。